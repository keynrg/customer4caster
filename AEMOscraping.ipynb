{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AEMOscraping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep19PzfuImw7",
        "colab_type": "text"
      },
      "source": [
        "#AEMO Scraping\n",
        "------\n",
        "This python code will scrape the \"NSW ELECTRICITY PRICE AND DEMAND\" table from AEMO <https://www.aemo.com.au/aemo/apps/visualisations/elec-nem-priceanddemand.html> through an API and save the table to your computer.\n",
        "\n",
        "It also supports scraping data of the other states by changing the parameter of regularScrape(region = 'NSW1') function to the abbreviation of the state, appending '1'. E.g VIC1 / QLD1 / SA1 / TAS1.\n",
        "\n",
        "To scrape the data automatically and periodically, run regularScrape(afterSeconds).\n",
        "\n",
        "Great thanks to facebook user Sirichai Sasataradol who contributed majoriliy to the code logic.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiiaclGZ1wiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime \n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "\n",
        "def scrape(region = 'NSW1', interval = \"30MIN\"):\n",
        "    #####################################################\n",
        "    # This function will scrape the data through the API#\n",
        "    # and save it to your computer.                     #\n",
        "    #####################################################\n",
        " \n",
        "    #1. Request & Scrape\n",
        "    s = requests.Session()\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36\", \n",
        "        \"Origin\": \"https://www.aemo.com.au\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Host\": \"www.aemo.com.au\",\n",
        "        \"Origin\": \"https://www.aemo.com.au\",\n",
        "        \"Referer\": \"https://www.aemo.com.au/aemo/apps/visualisations/elec-nem-priceanddemand.html\",\n",
        "        \"Sec-Fetch-Mode\": \"cors\",\n",
        "        \"Sec-Fetch-Site\": \"same-origin\"\n",
        "    }\n",
        "    s.headers = headers\n",
        "    json = {\"timeScale\": [interval]}\n",
        "    url = \"https://www.aemo.com.au/aemo/apps/api/report/5MIN\"\n",
        "    response = s.post(url, json=json)\n",
        "    results = response.json()\n",
        "    #Although it shows '5MIN', we are actually scraping the 30MIN data.\n",
        "    history = results['5MIN'] \n",
        "\n",
        "    #2. Make dataframe\n",
        "    data = [row.values() for row in history]\n",
        "    columns = history[0].keys()\n",
        "    df = pd.DataFrame(data, columns=columns)\n",
        "    df.index = df['SETTLEMENTDATE']\n",
        "    df = df[df['REGION']== region]\n",
        "    df = df.drop(columns = ['REGIONID', 'REGION','SETTLEMENTDATE', 'REGION'])\n",
        "    #The following line will save the data with its scraping time\n",
        "    df.to_csv(r'./{0}-{1}-{2}.csv'.format(datetime.datetime.now().strftime(\"%-d:%b:%y-%H%M\"),interval,region))\n",
        "    \n",
        "    print('{0} data archived.'.format(datetime.datetime.now().strftime(\"%-d/%b/%y - %H:%M\")))\n",
        "\n",
        "def regularScrape(afterSeconds = 1800, region = 'NSW1', interval = \"30MIN\"):\n",
        "    ##################################\n",
        "    #Specify the time to update data.#\n",
        "    ##################################   \n",
        "    while(True):\n",
        "        scrape(region, interval)\n",
        "        time.sleep(afterSeconds)\n",
        "        #After this, the loop will pause for afterSeconds seconds\n",
        "        #by default 30 minutes = 1800 seconds, unless specified.\n",
        "\n",
        "# On the board!!!\n",
        "regularScrape()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
