{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM Multisteps (2)",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_406eu9vpjjz",
        "colab_type": "text"
      },
      "source": [
        "**VERSION** LSTM Multistep v1.1 13-Nov-2019.\n",
        "Prototype date: In-Progress\n",
        "***\n",
        "**PROBLEM FRAME**\n",
        "\n",
        "*'Build a model that can forecast **4** values ahead'*\n",
        "\n",
        "**ACKNOWLEDGEMENT**\n",
        "\n",
        "*This notebook is heavily inspired by [Jason Brownlee](https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/)*.\n",
        "\n",
        "*Special thanks to Anand and Howard.* \n",
        "\n",
        "**NOTE**\n",
        "\n",
        "This is a back-up file, not a final version. It still cannot calculate RMSE and predict values.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEJIyuGeqZif",
        "colab_type": "text"
      },
      "source": [
        "**1. METHODS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr-cWQ9LQFRg",
        "colab_type": "text"
      },
      "source": [
        "a) Split methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y0sFAkvGBVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_dataset(dataset, fraction = 0.7):\n",
        "  ## Split the dataset into train and test set ##\n",
        "  train_size = int(len(dataset) * fraction)\n",
        "  test_size = len(dataset) - train_size\n",
        "  train_data, test_data = dataset[0:train_size,:], dataset[train_size:len(scaled_data),:]\n",
        "  return train_data, test_data\n",
        "\n",
        "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
        "\t## Split a univariate sequence into samples ##\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequence)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps_in\n",
        "\t\tout_end_ix = end_ix + n_steps_out\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif out_end_ix > len(sequence):\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWBMykkSQJiW",
        "colab_type": "text"
      },
      "source": [
        "b) Evaluation methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWieLxBAJQ5J",
        "colab_type": "text"
      },
      "source": [
        "**2. MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWMmEYukp641",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def build_model(X_train, y_train, n_steps_in=48, n_steps_out = 4, epochs=50, verbose=3, history = True): \n",
        "  ## Build an LSTM model ##\n",
        "  # Define model\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
        "  model.add(LSTM(100, activation='relu'))\n",
        "  model.add(Dense(n_steps_out))\n",
        "  model.compile(optimizer='adam', loss='mse')\n",
        "  # Train model\n",
        "  history = model.fit(X_train, y_train, epochs=epochs, verbose=verbose)\n",
        "  if(history): \n",
        "    print(history)\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCWdWb7dJZRv",
        "colab_type": "text"
      },
      "source": [
        "**3. IMPLEMENT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVLkr7jcUX7N",
        "colab_type": "text"
      },
      "source": [
        "a) Import the dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmOE84Xzp7Es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas import read_csv, to_datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "plt.rcParams['figure.figsize'] = 20, 10\n",
        "\n",
        "#Import the dataset\n",
        "dataset = read_csv(r'UTSusage.csv', header = 0, parse_dates = [['Date', 'Time']])\n",
        "dataset.Date_Time = to_datetime(dataset.Date_Time, format = '%Y/%m/%d %H.%M')\n",
        "dataset.index = dataset.Date_Time\n",
        "\n",
        "#Extract the needed column\n",
        "column_needed = ['DemandKva']\n",
        "dataset = dataset[column_needed]\n",
        "dataset = dataset.astype('float32')\n",
        "\n",
        "#Normalise the data\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_data = scaler.fit_transform(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMdcEN-hal0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "b12db436-f230-4f00-d028-b287b9955569"
      },
      "source": [
        "print(dataset.head())\n",
        "print(dataset.tail())\n",
        "print(dataset.describe())"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                      DemandKva\n",
            "Date_Time                      \n",
            "2019-01-01 00:00:00  131.070007\n",
            "2019-01-01 00:30:00  113.879997\n",
            "2019-01-01 01:00:00  109.949997\n",
            "2019-01-01 01:30:00  106.930000\n",
            "2019-01-01 02:00:00  124.400002\n",
            "                      DemandKva\n",
            "Date_Time                      \n",
            "2019-04-30 21:30:00  107.550003\n",
            "2019-04-30 22:00:00  109.989998\n",
            "2019-04-30 22:30:00  109.419998\n",
            "2019-04-30 23:00:00  109.519997\n",
            "2019-04-30 23:30:00  103.089996\n",
            "         DemandKva\n",
            "count  5760.000000\n",
            "mean    141.861221\n",
            "std      37.909058\n",
            "min      53.730000\n",
            "25%     116.157503\n",
            "50%     130.309998\n",
            "75%     160.449997\n",
            "max     380.269989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaG-jnqVR6Ct",
        "colab_type": "text"
      },
      "source": [
        "b) Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Tg-ko1B1m4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_steps_in= 48\n",
        "n_steps_out = 4\n",
        "# split dataset into train and test set\n",
        "train, test = split_dataset(scaled_data)\n",
        "# split into samples\n",
        "X_train, y_train = split_sequence(train, n_steps_in, n_steps_out)\n",
        "X_test, y_test = split_sequence(test, n_steps_in, n_steps_out)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtoeghKuSCkB",
        "colab_type": "text"
      },
      "source": [
        "c) Train LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqkJ5AdmRwqy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "124a967c-ef18-45e3-d125-8886ec710ba6"
      },
      "source": [
        "model = build_model(X_train, y_train, n_steps_in=48, n_steps_out = 4, epochs = 100, verbose = 10)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3980 samples\n",
            "Epoch 1/100\n",
            "...\n",
            "Epoch 100/100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gll4U3XSYUA",
        "colab_type": "text"
      },
      "source": [
        "c) Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkVS5mvrp6tj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "16fa9eaa-f7b6-4637-b7ed-78c5ccc485bd"
      },
      "source": [
        "# Demonstrate\n",
        "yhat = model.predict(X_test, verbose=10)\n",
        "print(scaler.inverse_transform(yhat))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[114.18548  115.56992  114.48395  116.33073 ]\n",
            " [115.55115  116.88935  115.77501  117.82671 ]\n",
            " [116.52499  117.55788  116.562584 118.39691 ]\n",
            " ...\n",
            " [112.55112  112.934166 110.60605  110.29754 ]\n",
            " [109.76127  110.34313  108.85847  109.34944 ]\n",
            " [107.45459  107.74198  107.079056 107.98145 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
